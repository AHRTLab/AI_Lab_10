{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U gym[atari,accept-rom-license]\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhvEpNQHA7f3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from wrappers import make_atari_env\n",
    "from replay_memory import ReplayBuffer\n",
    "from utils_modif import train_modif, test_modif\n",
    "from Param_modif import *\n",
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04A88YyNGAzz"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v-QcSh9GCS0",
    "outputId": "a1f62a66-fd89-47e8-ee14-1783487d6060"
   },
   "outputs": [],
   "source": [
    "'''Sets device to CPU or GPU'''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We can create our own network classes using the nn.Module'''\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        '''Set number of input and output neurons'''\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        '''We use the sequential method similar to Keras\n",
    "           We can also set two variables: conv and fc. This allows us to easily\n",
    "           our feature extractor (CNN) from our classifier (FC).'''\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''We need to define a forward method for predictions.\n",
    "           This method uses our self.conv network, flattens using x.view\n",
    "           and then outputs the final prediction using self.fc'''\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def act(self, state, epsilon, device=device):\n",
    "        '''Epsilon greedy policy implementation'''\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong!\n",
    "Now we are going to play some pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAZESkcdGDvv",
    "outputId": "825aea5d-dae7-497f-96a1-dcbb0f2352a2"
   },
   "outputs": [],
   "source": [
    "# We can create our Pong Environment and view the actionspace\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari_env(env_id)\n",
    "print(env.action_space)\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yo7qBUV3GFK_",
    "outputId": "811d760a-ecd5-4d43-9e40-47780df2ccbb"
   },
   "outputs": [],
   "source": [
    "'''This RL approach uses two models. We did not discuss this in class,\n",
    "   but a two model approach is typical'''\n",
    "current_model = DQN(env.observation_space.shape, env.action_space.n).to(device) # We tell pytorch where to run our model via .to    \n",
    "target_model = DQN(env.observation_space.shape, env.action_space.n).to(device)    \n",
    "\n",
    "'''Set the optimizer and Replay Buffer'''\n",
    "optimizer = optim.Adam(current_model.parameters(), lr=0.0001)\n",
    "replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "\n",
    "'''Load my saved model'''\n",
    "current_model.load_state_dict(torch.load('models_modified/PongNoFrameskip-v4_awesome.pth', map_location=torch.device('cpu')))\n",
    "target_model.load_state_dict(torch.load('models_modified/PongNoFrameskip-v4_awesome.pth', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "'''Play a (hopefully) perfect game of pong.'''\n",
    "model = current_model # set what model to use\n",
    "env.viewer = None     # Reset the environment viewer\n",
    "state = env.reset()   # Restart pong\n",
    "episode_reward = 0.0  # Reset Reward\n",
    "while True:\n",
    "    action = model.act(state, 0, device) # Determine what action to take\n",
    "    next_state, reward, done, _ = env.step(action) # Step through the env\n",
    "    env.render() # Render frame\n",
    "    time.sleep(0.02) # Small delay\n",
    "    episode_reward += reward # Calculate total reward\n",
    "    state = next_state # Next state\n",
    "    if done:\n",
    "        print(f\"Finished Episode with reward {episode_reward}\")\n",
    "        break\n",
    "\n",
    "env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping\n",
    "In order to look at rewards, we are going to use a wrapper to change Pong's rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Gym.RewardWrapper overwrites an environments reward function'''\n",
    "\n",
    "'''Change the reward function below so that the perfect agent learns how to lose'''\n",
    "class StupidRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Change reward function here\"\"\"\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "UUokYwVHGGoD",
    "outputId": "01d22013-c57c-459d-ee51-e650935e986d"
   },
   "outputs": [],
   "source": [
    "'''Now we can wrap the environment with our reward function'''\n",
    "env = make_atari_env(env_id)\n",
    "'''Your code here'''\n",
    "\n",
    "'''Stop coding here'''\n",
    "replay_buffer = ReplayBuffer(MEMORY_SIZE) # reset the replay buffer\n",
    "train_modif(env, current_model,target_model, optimizer, replay_buffer, device) # train for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we can view the new agent'''\n",
    "model = current_model\n",
    "env.viewer = None\n",
    "state = env.reset()\n",
    "episode_reward = 0.0\n",
    "while True:\n",
    "    action = model.act(state, 0, device)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "    episode_reward += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(f\"Finished Episode with reward {episode_reward}\")\n",
    "        break\n",
    "\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent \n",
    "In the cell below, write code that has your agent take random actions. You are welcome to use the code above and adapt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is the DQN architechture?\n",
    "2. What does each output neuron represent?\n",
    "3. Explain what a replay buffer is (you can google this).\n",
    "4. Explain the behavior of the perfect agent?.\n",
    "5. What the environments reward function before you changed it (you may need to look at the documentation)?\n",
    "6. Explain how you changed the reward function to get the agent to lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMss1OHMYeU5c2meGgJKPAD",
   "include_colab_link": true,
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
